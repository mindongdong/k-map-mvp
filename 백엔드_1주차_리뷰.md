  K-map 프로젝트: 1주차 스프린트 리뷰 (백엔드)

  1. 스프린트 목표 및 달성 여부

   * 1주차 목표: 핵심 설계 및 기본 골격 구현
       * [설계] API 명세서 작성 (Swagger/OpenAPI)
       * [구현] 데이터셋 API 기본 구현 (임시 데이터)
   * 달성 여부: 100% 완료

  2. 주요 작업 상세 내용

  A. API 설계 및 명세 완성

  FastAPI의 자동 문서 생성 기능을 활용하여, 기능 명세서에 정의된 모든 API의 명세를 코드 레벨에서 완성했습니다. 이를 통해 프론트엔드 팀은 별도의 문서 없이 http://localhost:8000/docs 에서 실시간으로 API의 상세 스펙(경로, 파라미터, 요청/응답 형식 등)을
  확인할 수 있습니다.

   * 구현된 전체 API 엔드포인트 목록:
       * Datasets
           * GET /api/v1/datasets: 모든 데이터셋 조회
           * GET /api/v1/datasets/{dataset_id}: 특정 데이터셋 조회
           * GET /api/v1/datasets/{dataset_id}/download/{file_name}: 파일 다운로드
       * Visualizations
           * GET /api/v1/visualizations/{chart_type}: 시각화 데이터 조회
       * Admin
           * POST /api/v1/admin/login: 관리자 로그인
           * POST /api/v1/admin/datasets: 데이터셋 생성
           * PUT /api/v1/admin/datasets/{public_dataset_id}: 데이터셋 수정
           * DELETE /api/v1/admin/datasets/{public_dataset_id}: 데이터셋 삭제
   * 상태: 모든 엔드포인트는 임시(mock) 데이터를 반환하며, 즉시 프론트엔드 연동 및 테스트가 가능한 상태입니다.

  B. 백엔드 기본 아키텍처 및 구조 설계

  개발_가이드.md와 src 참조 코드를 기반으로, 확장 가능하고 유지보수가 용이한 구조를 설계하고 구현했습니다.

   * 프로젝트 구조:
       * app/core: 환경변수, 데이터베이스 연결 등 핵심 설정
       * app/models: 데이터베이스 테이블과 매핑되는 SQLAlchemy ORM 모델
       * app/schemas: API의 입출력 데이터 형식을 정의하는 Pydantic 스키마
       * app/api: 기능별로 분리된 API 라우터 (엔드포인트)
   * 기술 스택 적용: FastAPI, Pydantic, SQLAlchemy

  C. 데이터베이스 모델링

  ERD.txt를 기반으로 프로젝트의 핵심 데이터인 User와 Dataset의 관계를 데이터베이스 모델로 상세하게 구현했습니다.

   * `models/user.py`: users 테이블 모델. 향후 데이터 업로더를 추적하기 위한 기반을 마련했습니다.
   * `models/dataset.py`: datasets 테이블 모델. public_dataset_id, group_name, data_type 등 ERD의 모든 컬럼과 users와의 관계(Foreign Key)를 정의했습니다.

  D. 환경 설정 및 실행 환경 구축

   * 환경변수 관리: pydantic-settings를 사용하여 .env 파일로 프로젝트의 모든 설정(DB 정보, JWT 비밀키 등)을 관리하도록 구현했습니다.
   * Docker 실행 환경: docker-compose.yml을 통해 백엔드 서버와 PostgreSQL 데이터베이스를 원클릭으로 실행할 수 있는 환경을 구축하고, 여러 실행 에러(ValidationError, ImportError, NameError)를 모두 해결하여 안정적인 실행을 보장합니다.

  3. 현재 상태 및 테스트 방법

   * 현재 상태: 백엔드 서버는 Docker를 통해 즉시 실행 가능한 상태입니다. 모든 API는 임시 데이터를 정상적으로 반환합니다.
   * 로컬 테스트 방법:
       1. 터미널에서 cp backend/.env.example backend/.env 실행
       2. 터미널에서 docker compose up --build backend db 실행
       3. 웹 브라우저에서 http://localhost:8000/docs 접속하여 API 테스트

  4. 다음 스프린트 (2주차) 계획

   * 현재 임시 데이터를 반환하는 API 로직을 실제 PostgreSQL 데이터베이스와 연동하여 데이터를 조회, 생성, 수정, 삭제하도록 구현할 예정입니다.
   * repository 디자인 패턴을 적용하여 API 로직과 데이터베이스 쿼리 로직을 분리하고 코드의 재사용성과 테스트 용이성을 높일 계획입니다.


K-map 프로젝트: 1주차 스프린트 상세 보고 (백엔드)

  1. 스프린트 목표 및 결과 요약

  1주차 목표는 API 명세 기반의 백엔드 기본 골격 구현이었습니다. FastAPI와 Pydantic의 기능을 적극 활용하여, 코드 자체가 명세가 되는 방식으로 전체 API 엔드포인트를 구현했으며, SQLAlchemy를 통해 데이터베이스 모델링을 완료하여 2주차 DB 연동을 위한 기반을
  마련했습니다.

  2. 코드 레벨 상세 작업 내용

  A. API 엔드포인트 및 명세 구현 (`app/api/`)

  기능 명세서의 모든 경로를 FastAPI의 APIRouter를 사용하여 기능별로 모듈화하고, 각 엔드포인트의 명세를 코드 레벨에서 정의했습니다.

   * `api/datasets.py`:
       * GET /datasets와 GET /datasets/{dataset_id} 엔드포인트를 구현했습니다.
       * 경로 매개변수는 타입 힌트(dataset_id: int)를 통해 자동으로 유효성 검사가 이루어집니다.
       * 응답 형식은 데코레이터(@router.get(..., response_model=DatasetSchema))에 Pydantic 스키마를 지정하여, FastAPI가 응답 본문을 자동으로 직렬화하고 API 문서에 명시하도록 구현했습니다.
       * 현재는 아래와 같이 하드코딩된 임시 데이터를 반환하여 즉시 테스트가 가능합니다.

   1         # api/datasets.py 예시
   2         @router.get("/{dataset_id}", response_model=DatasetSchema)
   3         def get_dataset_by_id(dataset_id: int):
   4             # ... (임시 데이터 조회 로직)
   5             if dataset:
   6                 return dataset
   7             raise HTTPException(status_code=404, detail="Dataset not found")

   * `api/admin.py` 및 `api/visualizations.py`:
       * POST, PUT, DELETE 등 나머지 엔드포인트들도 위와 동일한 방식으로 골격을 구현하여, 전체 API 명세가 /docs를 통해 제공되도록 완성했습니다.

  B. 데이터베이스 모델링 (`app/models/`)

  ERD.txt를 기반으로 SQLAlchemy의 ORM을 사용하여 데이터베이스 테이블 구조를 파이썬 클래스로 정의했습니다.

   * `models/dataset.py` & `models/user.py`:
       * Dataset과 User 클래스를 Base를 상속하여 생성하고, __tablename__으로 실제 DB 테이블 이름을 지정했습니다.
       * 테이블의 각 컬럼은 Column(String, Integer, Date) 등으로 상세 타입을 지정했습니다.
       * Dataset 모델의 uploader_id 컬럼에 ForeignKey("users.user_id")를 설정하여 데이터베이스 레벨의 관계를 정의했습니다.
       * ORM 레벨의 양방향 관계는 relationship을 사용하여 설정했으며, back_populates 인자를 통해 두 모델이 서로를 참조할 수 있도록 구현했습니다.

    1         # models/dataset.py 예시
    2         class Dataset(Base):
    3             # ...
    4             uploader_id = Column(Integer, ForeignKey("users.user_id"))
    5             uploader = relationship("User", back_populates="datasets")
    6 
    7         # models/user.py 예시
    8         class User(Base):
    9             # ...
   10             datasets = relationship("Dataset", back_populates="uploader")

  C. API 데이터 스키마 정의 (`app/schemas/`)

  Pydantic의 BaseModel을 상속받아 API의 요청(Request) 및 응답(Response) 본문의 데이터 구조를 엄격하게 정의했습니다.

   * `schemas/dataset.py`:
       * `DatasetSchema`: DB에서 조회한 데이터를 API 응답용으로 변환할 때 사용하는 스키마입니다. Config 클래스에 orm_mode = True를 설정하여, SQLAlchemy 모델 객체를 Pydantic 모델로 자동 변환할 수 있게 했습니다.
       * `DatasetCreate`, `DatasetUpdate`: 각각 데이터 생성(POST)과 수정(PUT) 시 요청 본문의 유효성을 검사하기 위한 스키마입니다. 이를 통해 API는 필요한 필드만 선택적으로 받을 수 있습니다.

    1         # schemas/dataset.py 예시
    2         class DatasetSchema(BaseModel):
    3             dataset_id: int
    4             public_dataset_id: str
    5             # ...
    6             class Config:
    7                 orm_mode = True # SQLAlchemy 모델 객체와 호환
    8
    9         class DatasetCreate(BaseModel):
   10             public_dataset_id: str = Field(...)
   11             # ... (생성 시 필요한 필드만 정의)

  D. 환경 설정 및 에러 해결 (`app/core/config.py`)

  Docker 환경에서 발생하는 여러 실행 에러를 해결하고 안정적인 실행 환경을 구축했습니다.

   * `core/config.py`:
       * `Pydantic ValidationError` 해결: .env 파일에 정의된 모든 환경변수(ALGORITHM, CORS_ORIGINS 등)를 Settings 클래스에 명시적으로 필드로 추가했습니다. 이를 통해 pydantic v2의 엄격한 유효성 검사를 통과하고, 설정 항목을 코드로 명확하게 관리할 수 있게
         되었습니다.
       * `ImportError` 및 `NameError` 해결: DatasetResponse라는 존재하지 않는 스키마를 참조하던 여러 파일(api/admin.py, schemas/__init__.py)을 수정하여, DatasetSchema로 참조를 통일함으로써 임포트 에러 체인을 완전히 해결했습니다.

  3. 결론 및 다음 단계

  1주차에는 API의 전체 구조와 명세를 코드로 완성하고, 데이터베이스 모델링을 통해 백엔드의 청사진을 그렸습니다. 모든 코드는 Docker 환경에서 즉시 실행 및 테스트가 가능합니다.

  2주차에는 api 계층에 구현된 임시 데이터 로직을 repository 계층으로 분리하고, 실제 PostgreSQL DB를 연동하여 영속성 있는 데이터를 다루는 작업을 진행할 것입니다.